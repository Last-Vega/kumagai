{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2544ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import re\n",
    "import sys\n",
    "import itertools\n",
    "import MeCab\n",
    "import matplotlib.pyplot as plt\n",
    "import MeCab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b362bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"特許リスト(21.12.6).xlsx\",header=0) # engineは環境によってエラー出たり出なかったりだから、必要に応じて変える\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500dc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['詳細な説明'] = df['詳細な説明'].fillna(' ') #nanを文字列に\n",
    "df['全請求項'] = df['全請求項'].fillna(' ')\n",
    "df[\"company\"] = [set(i.split(\",\")) for i in df['出願人・権利者名']]    #出願名をスプリットしてset型に\n",
    "df['datetime'] = [dt.strptime(i.split(',')[0],  '%Y.%m.%d') for i in df['公開・公表日']]  #公開・公表日を datetime64[ns] 型（ Timestamp 型）に 、複数あるのは先頭のだけに\n",
    "# df['sentence'] = df['発明の名称'] +df['要約'] + df['全請求項'] .astype(\"str\") + df['詳細な説明']  #なぜか .astype(\"str\") いれないとエラー\n",
    "# df['sentence'] = df['発明の名称'] +df['要約'] + df['詳細な説明']  \n",
    "# df['sentence'] = df['発明の名称'] +df['要約']        #sentenceの対象にする項目も必要に応じて変える\n",
    "df['sentence'] = df['詳細な説明']        #sentenceの対象にする項目も必要に応じて変える\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825e3cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WatanabeShingo/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "excepword = [\"位置\",\"部\",\"上記\",\"上部\",\"上端\",\"下記\",\"下部\",\"下端\",\"両端\",\"内部\",\"外部\",\"前記\",\"複数\",\"横\",\"図\",\"要約\",\"方法\",\"手段\",\"対象\",\"現場\",\"評価\" ,\"可能\",\"構成\",\"当該\",\"以下\",\"以上\",\"該当\",\"項\",\"各\",\"基準\",\"端部\",\"請求\", \"特許\",\"本\",\"一定\",\"同上\"]  #除外する特定の名詞\n",
    "exceptype= [\"接尾\",\"副詞可能\",\"数詞\",\"助数詞\",\"代名詞\",\"複数\"]  #除外する名詞のタイプ\n",
    "\n",
    "\n",
    "# \n",
    "def tokenize_rm_setsubi(sentence):\n",
    "    \"\"\" 連名詞のリストをトークンとして返す \"\"\"\n",
    "#     print(\".\",end='') #動いてるのか不安になるからプリント\n",
    "    sentence  = re.sub('\\d+', '', sentence)   # 数字を削除(macabでなぜか名詞に認定された数字あったからここで)\n",
    "    sentence  = text=re.sub(r'[ -/:-@\\[-~]', \"。\", sentence)#半角記号,数字,英字\n",
    "    sentence  = text=re.sub(r'[！-／：-＠［-｀｛-～、-〜”’%]+/g', \"。\", sentence)#全角記号\n",
    "    \n",
    "    \n",
    "    \n",
    "    wakati = MeCab.Tagger(\"\")\n",
    "    node = wakati.parseToNode(sentence)\n",
    "    sequence = 2   #何連名詞以上をかえすか\n",
    "    dictionary = []\n",
    "    prev_seq = 0  #その前にいくつ名詞続いたか\n",
    "    flag = 0  #\n",
    "    while node:\n",
    "        word = node.surface\n",
    "        hinshi = node.feature.split(\",\")\n",
    "\n",
    "        if prev_seq  : # その前に名詞が1以上つづいていたら\n",
    "            if  (hinshi[0] == \"名詞\" ) and  (hinshi[0] not in exceptype) and  (hinshi[1] not in exceptype )and (hinshi[2] not in exceptype )and (word not in excepword) : #今回、除外名詞以外だったら、\n",
    "                dictionary[-1] = dictionary[-1]+word   # 辞書の最後の単語にこの単語を結合\n",
    "                prev_seq  +=1\n",
    "                flag = 0\n",
    "                \n",
    "            elif (hinshi[1] == \"接尾\" ): #接尾辞だったら\n",
    "                dictionary[-1] = dictionary[-1]+word   # とりあえず辞書の最後の単語にこの単語を結合して\n",
    "                prev_seq  +=1\n",
    "                flag = 1 #フラグを1に\n",
    "\n",
    "            elif flag == 1: # 一個前が接尾辞で今度が名詞でなかったら、一個前の接尾辞で終わってる単語を辞書から削除\n",
    "                dictionary.pop(-1)   #一個前の接尾辞で終わってる単語を辞書から削除\n",
    "                prev_seq  =0\n",
    "                flag = 0\n",
    "            else:\n",
    "                flag = 0\n",
    "                if prev_seq  < sequence: \n",
    "                    dictionary.pop(-1) #一個前の単語を辞書から削除\n",
    "                prev_seq = 0\n",
    "        else:  # その前に名詞がつづいてなくて\n",
    "            if  (hinshi[0] == \"名詞\" or hinshi[0] == \"接頭辞\")  and  hinshi[0] not in exceptype and  hinshi[1] not in exceptype and hinshi[2] not in exceptype and word not in excepword :\n",
    "                dictionary.append(word)\n",
    "                prev_seq += 1\n",
    "                flag = 0\n",
    "\n",
    "        node = node.next  \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "\n",
    "# 単語ー文書行列の作成\n",
    "docs = np.array(df['sentence'])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer = tokenize_rm_setsubi, #上でつくった関数\n",
    "    lowercase=True,\n",
    "    max_df=3000,\n",
    "#     smooth_idf = False,\n",
    "    min_df=10)\n",
    "\n",
    "tfidf_vectorizer.fit(docs)\n",
    "terms= tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "count_vectrizer = CountVectorizer(\n",
    "    vocabulary = terms,\n",
    "    tokenizer = tokenize_rm_setsubi, #上でつくった関数\n",
    ")\n",
    "\n",
    "c_matrix = count_vectrizer.transform(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901be01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e37874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('c_matrix_allterm',c_matrix)\n",
    "sp.sparse.save_npz('c_matrix_allterm',c_matrix)\n",
    "terms_df = pd.DataFrame(terms)\n",
    "terms_df.to_csv('terms_df.csv', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48239b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = sp.sparse.load_npz('c_matrix_allterm.npz')\n",
    "terms_df = pd.read_csv('terms_df.csv')\n",
    "terms = terms_df['0'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e31bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>document_frequency</th>\n",
       "      <th>max_term_frequency</th>\n",
       "      <th>sumof_term_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12332</th>\n",
       "      <td>図示例</td>\n",
       "      <td>2813</td>\n",
       "      <td>45</td>\n",
       "      <td>9155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7513</th>\n",
       "      <td>作業効率</td>\n",
       "      <td>2766</td>\n",
       "      <td>25</td>\n",
       "      <td>6254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12280</th>\n",
       "      <td>回転自在</td>\n",
       "      <td>2645</td>\n",
       "      <td>45</td>\n",
       "      <td>8078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12340</th>\n",
       "      <td>図示省略</td>\n",
       "      <td>2637</td>\n",
       "      <td>24</td>\n",
       "      <td>5965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16415</th>\n",
       "      <td>延在</td>\n",
       "      <td>2595</td>\n",
       "      <td>68</td>\n",
       "      <td>9835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5772</th>\n",
       "      <td>三次元構造</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27675</th>\n",
       "      <td>管自体</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5773</th>\n",
       "      <td>三次元点群データ</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27660</th>\n",
       "      <td>管理用パソコン</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>仕上げパネル</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36759 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          terms  document_frequency  max_term_frequency  sumof_term_frequency\n",
       "12332       図示例                2813                  45                  9155\n",
       "7513       作業効率                2766                  25                  6254\n",
       "12280      回転自在                2645                  45                  8078\n",
       "12340      図示省略                2637                  24                  5965\n",
       "16415        延在                2595                  68                  9835\n",
       "...         ...                 ...                 ...                   ...\n",
       "5772      三次元構造                  10                   8                    34\n",
       "27675       管自体                  10                   3                    13\n",
       "5773   三次元点群データ                  10                  28                    67\n",
       "27660   管理用パソコン                  10                  25                    57\n",
       "7007     仕上げパネル                  10                  33                    52\n",
       "\n",
       "[36759 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_df = pd.DataFrame(terms, columns=[\"terms\"])\n",
    "c_df[\"document_frequency\"] =  np.array(c_matrix.astype(bool).sum(axis=0).reshape(-1,1))\n",
    "c_df[\"max_term_frequency\"]  = c_matrix.T.max(axis=1).toarray()\n",
    "c_df[\"sumof_term_frequency\"] = np.array(c_matrix.T.sum(axis=1))\n",
    "c_df.sort_values('document_frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c78c941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#単語:文書内単語頻度 の辞書の列つくって、pandasにくっつける\n",
    "def makeTermsTfDict(terms, vec):\n",
    "    # dic = {key: val for key, val in zip(terms, vec)  if val } #dfが1より大きいtermの辞書つくる\n",
    "    dic = {key: val for key, val in zip(terms, vec)}\n",
    "    dic2 = {k: v for k, v in dic.items() if v != 0}\n",
    "    dic3 = dict(sorted(dic2.items(), key=lambda x:x[1],reverse=True)) #dfの値でソート\n",
    "\n",
    "    return dic3\n",
    "\n",
    "\n",
    "dic = makeTermsTfDict(terms, c_matrix.getrow(1).toarray()[0]) \n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    df.at[i,'terms'] = str(makeTermsTfDict(terms, c_matrix.getrow(i).toarray()[0] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c1c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('parcedPatents1217.xlsx', sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe680ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asd = pd.read_json('parcedPatents.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = eval(asd.at[0,'terms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f372a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd18fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
